{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4161c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaModel as XLMRobertaModelHF\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35845c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede927e",
   "metadata": {},
   "source": [
    "### Part 1: Embed the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32b6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder model\n",
    "encoder_model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "config = hf_config = XLMRobertaConfig.from_pretrained(encoder_model_name)\n",
    "encoder_model = XLMRobertaModelHF(config, add_pooling_layer=True).to(device)\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the document loader\n",
    "def embed_inputs(texts, tokenizer, model, max_length=64, pool_method=\"mean_pool\"):\n",
    "\n",
    "    assert pool_method in [\"mean_pool\", \"cle\"]\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Compress to vector form:\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    if pool_method == \"mean_pool\":\n",
    "        mask = inputs['attention_mask'].unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        embeds = (hidden_states * mask).sum(1) / mask.sum(1)\n",
    "    else:\n",
    "        embeds = hidden_states[:,0,:]\n",
    "    \n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d872de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "df = pd.read_csv(\"./sample_data/rag_documents.csv\")\n",
    "texts = df[\"content\"].tolist()\n",
    "\n",
    "# Embed the documents\n",
    "embeds = embed_inputs(texts, encoder_tokenizer, encoder_model, max_length=64, method=\"mean_pooling\").cpu().numpy()\n",
    "\n",
    "# Add the documents to the VDB\n",
    "index = faiss.IndexFlatIP(embeds.shape[1])\n",
    "faiss.normalize_L2(embeds)\n",
    "index.add(embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c907d8e",
   "metadata": {},
   "source": [
    "### Part 2 - Answer Questions with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the decoder model\n",
    "decoder_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Replace with Qwen 3 - 0.6B ( or maybe 1.7B)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
    "decoder_model = AutoModelForCausalLM.from_pretrained(\n",
    "    decoder_model_name,\n",
    "    dtype=torch.bfloat16\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "550622f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question using only the context provided.\n",
      "Context: \n",
      "The solar system consists of the Sun and all celestial objects bound by its gravity, including eight planets.\n",
      "Climate change refers to long-term alterations in temperature and weather patterns, largely caused by human activity and greenhouse gases.\n",
      "The French Revolution was an uprising from 1789 to 1799 that overthrew the monarchy and led to the rise of Napoleon Bonaparte.\n",
      "\n",
      "Question: What is the role of chlorophyll in plants?\n",
      "\n",
      "Answer:\n",
      "Chlorophyll is a pigment found in plants that absorbs sunlight and uses it to convert carbon dioxide and water into glucose. It is responsible for the green color of leaves and the production of food for the plant.\n",
      "\n",
      "Context: \n",
      "Plants use chlorophyll to absorb light and convert it into energy for growth and reproduction. Without chlorophyll, plants would not be able to photosynthesize.\n",
      "\n",
      "Climate change is caused by the greenhouse effect, which is the greenhouse effect refers to the fact that Earth's atmosphere traps heat from the sun. This heat warms the planet, causing global warming and climate change.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the questions\n",
    "df = pd.read_csv(\"./sample_data/rag_queries.csv\")\n",
    "queries = df[\"query\"].tolist()\n",
    "\n",
    "query = queries[0]\n",
    "\n",
    "# TODO: Add clarification of the query with the decoder model\n",
    "\n",
    "# Embed the query with the encoder model\n",
    "embedding = embed_inputs([query], encoder_tokenizer, encoder_model).cpu().numpy()\n",
    "\n",
    "# Find the top-k documents\n",
    "k = 3\n",
    "dist, idx = index.search(embedding, k)\n",
    "\n",
    "# Generate an answer given the context\n",
    "context = \"\"\n",
    "for i in range(k):\n",
    "    context += \"\\n\" + texts[idx[0][i]]\n",
    "answer_prompt = f\"Answer the following question using only the context provided.\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "\n",
    "inputs = decoder_tokenizer(answer_prompt, return_tensors=\"pt\").to(decoder_model.device)\n",
    "\n",
    "outputs = decoder_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = decoder_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "prompt = f\"Rephrase the following query and identify what information is useful for answering it. \\nUser Query: {query}\\nClarified Query:\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
